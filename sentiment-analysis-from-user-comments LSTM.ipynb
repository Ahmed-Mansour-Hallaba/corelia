{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "66fab5b194e718848490953a27a1e33db33b531c",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "comments_data = pd.read_csv('Tweets1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "314b63bf5806303214383b4f2d788bcaf97bedbc",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet\n",
       "0   neutral                @VirginAmerica What @dhepburn said.\n",
       "1  positive  @VirginAmerica plus you've added commercials t...\n",
       "2   neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3  negative  @VirginAmerica it's really aggressive to blast...\n",
       "4  negative  @VirginAmerica and it's a really big bad thing...\n",
       "5  negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6  positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "7   neutral  @VirginAmerica Really missed a prime opportuni...\n",
       "8  positive    @virginamerica Well, I didn't…but NOW I DO! :-D\n",
       "9  positive  @VirginAmerica it was amazing, and arrived an ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_data.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "c1b62381b192a1c5ab4fea94b7df20008cc44c7f",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "b1bffcfe1ffd21d4dbd2166a9005e826b7a2e9df",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# extract sentiment(positive/negative/neutral) into labels\n",
    "labels = comments_data[\"label\"].map({'neutral':0,'negative':1,'positive':2})\n",
    "labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "# extract text to be analysed into comments\n",
    "comments = comments_data[\"tweet\"]\n",
    "#remove words which are starts with @ symbols\n",
    "comments = comments.map(lambda x: \" \".join(str(x).split()))\n",
    "comments = comments.map(lambda x:re.sub('@\\w*','',str(x)))\n",
    "#remove special characters except [a-zA-Z]\n",
    "comments = comments.map(lambda x:re.sub('[^a-zA-Z]',' ',str(x)))\n",
    "#remove link starts with https\n",
    "comments = comments.map(lambda x:re.sub('http.*','',str(x)))\n",
    "#convert to ndarray\n",
    "comments = np.asarray(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "b3e1f6d53c8b8cde594eacce91ea06272532cebe",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What  said  0\n"
     ]
    }
   ],
   "source": [
    "# Change index value to see comments and corresponding sentiment type\n",
    "index = 0\n",
    "print(comments[index], labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "6ff1e5fff8a41908697abf38509575602241d09a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find number of unique categories\n",
    "num_classes = comments_data[\"label\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f5409584c9bd499491f85070cbddea6c1dca0c29",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Convert to one-hot vector\n",
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "5c2093dfa256764aad43f7aa37346935d81b5387",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Y_one_hot = convert_to_one_hot(labels, C = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "e05ec4d0bc629a62500d0872c4f750d6992095fc",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and test data\n",
    "X_train,X_test,y_train,y_test = train_test_split(comments,Y_one_hot,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "72c9e7005ae0cebedadda9e449a3693dbcb45209",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11712,) (2928,) (11712, 3) (2928, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "12c1b32c053674af9475dcda8107a6176c0bb801",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "maxLen = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "667b00491d27f655bbfcbd56aed07b6e304043e4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Load Glove Vectors\n",
    "# GLOVE_DIR='../input/glove-global-vectors-for-word-representation/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "words = set()\n",
    "word_to_vec_map = {}\n",
    "for line in f:\n",
    "    values = line.strip().split()\n",
    "    word = values[0]\n",
    "    words.add(word)\n",
    "    word_to_vec_map[word] = np.array(values[1:], dtype=np.float64)\n",
    "    \n",
    "i = 1\n",
    "words_to_index = {}\n",
    "index_to_words = {}\n",
    "for w in sorted(words):\n",
    "    words_to_index[w] = i\n",
    "    index_to_words[i] = w\n",
    "    i = i + 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "fdd339f232abaae70834cf258a35ef64a5f1515e",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at a word embedding\n",
    "word_to_vec_map[\"hello\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "8b780b450d58b25190d0e2c4d8ca97b082296339",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"hello\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim,trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "ae9c3528e662e0eb33944205e1268c939f740fa0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][3] = -1.5434\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "print(\"weights[0][1][3] =\", embedding_layer.get_weights()[0][1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "4fe08502d0502368c7d21ab0aa060b9f7441b062",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        sentence_words = X[i].strip().lower().split(\" \")\n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            if w and w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "7727659a85fc2f2b941cfddc7d079dfae38b23e2",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "input_shape = (maxLen,)\n",
    "sentence_indices = Input(shape=input_shape,dtype='int32')\n",
    "    \n",
    "# Create the embedding layer pretrained with GloVe Vectors\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "    \n",
    "# Embedding layer, you get back the embeddings\n",
    "embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "# LSTM layer with 128-dimensional hidden state\n",
    "X = LSTM(128,return_sequences=True)(embeddings)\n",
    "X = Dropout(rate=0.5)(X)\n",
    "# LSTM layer with 128-dimensional hidden state\n",
    "X = LSTM(128)(X)\n",
    "X = Dropout(rate=0.5)(X)\n",
    "# Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "X = Dense(num_classes)(X)\n",
    "# Add a softmax activation\n",
    "X = Activation(\"softmax\")(X)\n",
    "    \n",
    "# Create Model instance which converts sentence_indices into X.\n",
    "model = Model(inputs=sentence_indices, outputs=X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "b59d24a1f38d92a2c9e32fb4ab2f3d2ea1064872",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 40)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 40, 100)           40000100  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 40, 128)           117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 40, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 387       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,249,319\n",
      "Trainable params: 249,219\n",
      "Non-trainable params: 40,000,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "ae3e361b9d235c48f225e94b59fbcf0474480064",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "93535a247c7d6ed397b452203db4449cbd645396",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, words_to_index, maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "f30f2d5020461f8f568990c88992d09127172614",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "366/366 [==============================] - 24s 58ms/step - loss: 0.7380 - accuracy: 0.6986\n",
      "Epoch 2/20\n",
      "366/366 [==============================] - 21s 58ms/step - loss: 0.6192 - accuracy: 0.7559\n",
      "Epoch 3/20\n",
      "366/366 [==============================] - 21s 59ms/step - loss: 0.5655 - accuracy: 0.7755\n",
      "Epoch 4/20\n",
      "366/366 [==============================] - 22s 59ms/step - loss: 0.5290 - accuracy: 0.7985\n",
      "Epoch 5/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.5153 - accuracy: 0.8043\n",
      "Epoch 6/20\n",
      "366/366 [==============================] - 22s 60ms/step - loss: 0.4838 - accuracy: 0.8164\n",
      "Epoch 7/20\n",
      "366/366 [==============================] - 22s 59ms/step - loss: 0.4593 - accuracy: 0.8241\n",
      "Epoch 8/20\n",
      "366/366 [==============================] - 22s 59ms/step - loss: 0.4294 - accuracy: 0.8363\n",
      "Epoch 9/20\n",
      "366/366 [==============================] - 22s 59ms/step - loss: 0.4179 - accuracy: 0.8438\n",
      "Epoch 10/20\n",
      "366/366 [==============================] - 22s 59ms/step - loss: 0.3874 - accuracy: 0.8579\n",
      "Epoch 11/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.3675 - accuracy: 0.8687\n",
      "Epoch 12/20\n",
      "366/366 [==============================] - 23s 62ms/step - loss: 0.3407 - accuracy: 0.8756\n",
      "Epoch 13/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.3121 - accuracy: 0.8860\n",
      "Epoch 14/20\n",
      "366/366 [==============================] - 23s 62ms/step - loss: 0.2884 - accuracy: 0.8928\n",
      "Epoch 15/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.2603 - accuracy: 0.90700s - loss: 0.2604 - accuracy: \n",
      "Epoch 16/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.2461 - accuracy: 0.9148\n",
      "Epoch 17/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.2234 - accuracy: 0.9226\n",
      "Epoch 18/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.2106 - accuracy: 0.9269\n",
      "Epoch 19/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.1965 - accuracy: 0.9337\n",
      "Epoch 20/20\n",
      "366/366 [==============================] - 22s 61ms/step - loss: 0.1730 - accuracy: 0.9419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bb899bde20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, y_train, epochs = 20, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "0634218502386974d14f88ec7a4ab939f2bab8f7",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 3s 24ms/step - loss: 0.8237 - accuracy: 0.7787\n",
      "\n",
      "Test accuracy =  0.7786885499954224\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, words_to_index, maxLen)\n",
    "loss, acc = model.evaluate(X_test_indices, y_test)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "b607d4d84460e9abeb1173fdc215f0e0990ec117",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst experience ever::::Expected=1, Prediction=1\n",
      "it was a wonderful experience::::Expected=2, Prediction=2\n",
      "it was just fine::::Expected=2, Prediction=0\n",
      "hello how are you doing::::Expected=0, Prediction=0\n",
      "not a very good airline::::Expected=1, Prediction=2\n"
     ]
    }
   ],
   "source": [
    "#Show mis-classified comments\n",
    "test_data = np.asarray([\"worst experience ever\",\"it was a wonderful experience\",\"it was just fine\", \"hello how are you doing\", \"not a very good airline\"])\n",
    "test_data_y = np.asarray([1,2,2,0,1])\n",
    "test_indices = sentences_to_indices(test_data, words_to_index, maxLen)\n",
    "pred = model.predict(test_indices)\n",
    "for i in range(len(test_data)):\n",
    "    x = test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    act = test_data_y[i]\n",
    "    print(test_data[i] + '::::Expected='+ str(act) + ', Prediction=' + str(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe205be882d15b19adc6d0ab5ce88d7d2cb212c2",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
